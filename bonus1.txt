File to test: sports.txt
Trained files: health.txt, games.txt, news.txt, shopping.txt

Trials are in format (lambda0, lambda1, lambda2, lambda3):

Original (0.25, 0.25, 0.25, 0.25): 47.684067133
Trial 1 (0.10, 0.20, 0.30, 0.40): 37.8670358405
Trial 2 (0.05, 0.10, 0.35, 0.50): 34.9061759692
Trial 3 (0.05, 0.10, 0.50, 0.35): 36.0087203484
Trial 4 (0.0, 0.0, 0.0, 1.0): 4.78219861111
Trial 5 (0.0, 0.05, 0.10, 0.85): 40.0256769952

With the exception of the anomoly of solely using the trigram model, the values
of lambda that gave me the lowest perplexity are (0.05, 0.10, 0.35, 0.50), with
perplexity score 34.9061759692. My approach was to add more weight to the 
bigram and trigram models because these two models contain more information 
about the likelihood of a particular word appearing next given a history. This
improved my perplexity score because the bigram and trigram models carry more
information than the unigram and uniform models. 

Also, putting more weight on the trigram model than bigram model yielded a 
better result, perhaps because of the very same phenomenon described above. 
